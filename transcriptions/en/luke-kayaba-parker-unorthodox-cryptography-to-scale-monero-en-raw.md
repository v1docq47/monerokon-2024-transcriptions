#  Luke 'Kayaba' Parker

_**Unorthodox Cryptography to Scale Monero**_

_Monero is expected to evolve to Seraphis, adopt JAMTIS as our new address scheme, and even adopt Full Chain Membership Proofs for complete sender privacy. All of these rely on Elliptic Curve Cryptography, with the most notable construction being the curve cycle (an elliptic curve whose scalar field embeds the field of an elliptic curve whose scalar field is the original curve's field) proposed._

_This talk will go over alternate cryptographic schemes which can offer a more performant membership proof, decrease the storage requirements of Monero, eliminate the need to iteratively scan transactions for near-instant scan times, and increase privacy for users who don't run their own nodes._

[https://youtu.be/tHGVtGELMUU](https://youtu.be/tHGVtGELMUU)

---

_**Moderator:**_ So the next presentation will review alternate cryptographic schemes which can offer a more performant membership proof, decrease the storage requirements of Monero, eliminate the need to iteratively scan transactions and increase privacy for users who don't run their own nodes. So please welcome again Luke Parker.

_**Luke:**_ Hi, so for anyone who attended this morning, my name is Luke Parker. This is "Unorthodox Cryptography to Scale Monero".

Most of you all have already known these. I'm just gonna reiterate that I'm a MAGIC Monero Fund committee member, because MAGIC is a US 501(c)(3) charity. If you pay US taxes and wanna get a deduction on that for donating to Monero, it's a great opportunity to do so. And also, I would like to note how privileged I am to be a four-time speaker at MoneroKon. We'll see how that goes.

So, current Monero cryptography, as you may have heard in this last talk, which is really humorously placed in my opinion, is premastone elliptic curves. They're fast, efficient, and secure (in the pre-quantum world) groups, "groups" just being this mathematical notion. They underpin most modern cryptography. They're well studied, widely usable, and that's it. They work, and they work well. We could go back to RSA, but Monero cryptography would be 10 times larger and 100 times slower. Why would we do that when elliptic curves just work?

Well, what if we do want more? What if we want something that doesn't just work, really goes out there and kicks some ass? So I figured I'd start this by going over some examples of things we want and start talking about how we can do that, kick some ass.

So full-chain membership proofs. Currently we're looking at using Bulletproofs for that purpose. Bulletproofs are a proof we're using with elliptic curves, and they have an O(n)-time complexity and an O(log s)-size. If you're unfamiliar with big O notation, I'm so sorry. Honestly, this talk is gonna use it a bit. Basically, that means if your program does 100 things, it's gonna take 100 units of time to verify, but it's only going to take the space of 10 different things. It's much smaller in size, but yes, the time is equivalent.

So in the future, we could look at upgrading to different proofs that are also over elliptic curves such as Spartan. And the great thing about Spartan is that not only is it small in space, but it is also small in time. The bad thing about this is that Spartan is a takes longer to do each individual step, and because it would take longer to do each individual step, it may still not work out in practice, despite using less units. Each unit may be so much more expensive, it just doesn't work out.

So this is where things get a bit wordy. I apologize to everyone who's at the end of their day. But basically, instead of looking at log schemes, which are logarithmic, we could look at cryptographic accumulators. Cryptographic accumulators would only take O(1) in both space and time, regardless of how much data we're handling. They just take a very constant, very small, very fast, theoretically, to verify, all of that to verify. They also have a (log n) prover, which is very competitive and exactly what we would be looking for, assuming a global pre-process. So as your node is kind of iterating over outputs, it just does this pre-process, and then when it comes time to send a transaction, it's already ready.

So cryptographic accumulators would be this ideal way to do a full chain membership proof, but unfortunately they are historically premised on RSA, exactly what we don't want to be using in Monero. There would be 384 bytes per element instead of just 32 bytes per element, which we get with an elliptic curve. So it legitimately would be 10 times bigger, and we would need a trusted setup.

If we didn't use a trusted setup, we would be using a scheme known as RSA UFO. Unfortunately, it was not brought to us by aliens. RSA UFO means RSA unknown factorization, and it means that unlike a traditional RSA key where someone generates an RSA public key and releases it into the wilderness. We just randomly generate this super large number, because the super large number statistically is at some point in it going to have two prime numbers of sufficient length that no one knows. And in order to randomly generate a large number that happens to have these two very large prime numbers it would have to be 46 kilobytes which is 20, 30, 40 times larger than entire Monero transactions today. So that idea also doesn't really work out.

And that kind of leads to this one final note. We could use class groups. Class groups don't have a trusted setup, and they also aren't 46 kilobytes if they don't have a trusted setup. They're actually just a couple hundred bytes. And because they're a couple hundred bytes, the full proof around them would be roughly six times smaller than the currently proposed proof. And the currently proposed proof is linear. to verify with regards to the time, sorry, it's linear in time to verify compared to the size of the program, but this accumulator proof would be O(1). No matter how large our output set is, it would be a constant amount of time to prove that the output we're spending is in fact a member of the output set.

And from there it kind of breaks down pretty fast. Despite being O(1) to verify, we're already achieving logarithmic verification with Bulletproofs just because we're not encoding a linear program. We're encoding a logarithmic program. And it would still be much slower than the Bulletproof space proof in practice. So not only would it be slower in practice, well, it would still be notably smaller, except. Class groups have this really interesting property. If you want them to have 128 bits of security, they should only need a quote unquote 1800 bit discriminant. This discriminant is this, let's go with configuring number of the class group. The issue is that if you choose an 1800 bit discriminant, yeah, it issues 128 bits of security, except for very random chances where its security just completely goes off a cliff. And you won't actually know if its security went off a cliff until you actually try and break it. And yeah, sure, after spending 100 bits of effort to break it, you realize it was not, in fact, 128 bit.

So in order for a globally used, going to last the next 10 years, going to survive into the future class group to exist, you need to choose a much larger number because then the random chance its security happened to go off a cliff is improbable, it needs to be four to five times bigger. Still smaller than currently discussed proofs, but not nearly as appetizing as six times smaller.

And then also just practically, we'd have to implement all of this cryptography for class groups. They're not elliptic curves. It would be an entire set of libraries, entire set of audits. And then in the future, one of the things I wanna be very cautious about is meta-proofs and having designed amenable to meta-proofs, meta-proofs just being proofs about proofs, which are a way to gain greater efficiency in the future.

With elliptic curves, we know very well how to do these sort of quote-unquote meta-proofs. Specifically we'd be discussing a curve cycle, an elliptic curve whose scalar field is the field of another elliptic curve. It gets a bit complicated, but kind of the point is, yes, we know how to work with elliptic curves inside elliptic curves very efficiently. What we couldn't do is if we ever introduce a class group proof is we couldn't work with that class group proof inside an elliptic curve premise meta-proof. So years into the future, we would still be dealing with these class groups because we would end up needing to do a class group based meta-proof for the class group proofs.

So in the end, we actually could get this better theoretical properties and we could get smaller proofs for class groups but it would require a lot of effort and probably doesn't balance out.

For the next topic I wanted to approach, I wanted to actually discuss wallet syncing. One of the main complaints of Monero users, I mean, I'm sure if I asked for a raise of hands, anyone who's ever had issues syncing their wallet, we'd get a bunch. There's one, there's another.

And as of right now, you need to download every single transaction's outputs and run a scan process on it. So this is linear. As the amount of outputs go up, the amount of time the scan process goes, the amount of time this scan process takes goes up, as does your bandwidth, as you now actually have to download these outputs.

And when Monero was being spammed, this unfortunately became infeasible for some wallets and some servers. I know Cake actually had to do a lot of work on making their servers more efficient and responsive to this.

So once again, if we move from elliptic curve cryptography to technically not RSA, but it might as well be RSA, we could actually gain stealth addresses, which are O(1) to check. This means if you have a million outputs, you could immediately see if any of them were to you. And then if you do see that, oh, yes, one of these million are to you, you can immediately partition them. You can say, well, are these first 500,000, are these the ones that contain the output to me, or is it the second 500,000? Does this output contain the one to me? And within, if you have one output within a million, for that specific example, you would be looking at 25 or so, 25 requests to a wallet server to sync through 1 million outputs. So it would just be very efficient and allow wallets to very quickly find which outputs were actually belong to them.

Bad news, there actually is a scheme for this to be clear. It's a full protocol already proven and so on. It requires you register on chain, and that's just a UX pain. If a user creates a Monero address, they now have to register it on chain. The bigger issue is that the stealth addresses, they only require one attempted decryption to check, but the addresses aren't constant size as we know them today. Monero addresses are constant size. The one-time keys are always one elliptic curve point and always 32 bytes.

Unfortunately, for these stealth addresses, which only take a constant amount of time to check, regardless of how many outputs you're discussing, they're of constant size to the amount of potential receivers. So if we have a million outputs on chain, that stealth address is going to be gigabytes, or sorry megabytes actually, megabytes, I believe hundreds of megabytes, it's been in the middle. Which is horrific. No, we cannot have hundreds of megabytes go on chain with every output.

And also, it would not be forward secret, as currently posited. Forward secret sees something very important to me. It means that when a quantum computer comes around, it can't go back to the current Monero transactions and trace them. And this scheme, as posited, no, it would not be forward secret in the slightest. So on the one hand, it's a very interesting theoretical avenue. And it's truly fascinating that someone created any scheme that was able to claim a constant time check regardless of how many outputs were on chain, but in practice, no, this does not work out and still needs massive theoretical improvements for us to discuss.

Remote wallet syncing, this is kind of an adjacent idea. It actually goes to Jamtis, which I believe there's a talk on Sunday. I might be wrong about that. But one of the nice features about Jamtis is that you can actually delegate the syncing process. And not like what you can currently do with Monero today. Right now with Monero you can always hand over your view key and have someone find your outputs. With Jamtis, you can hand someone a specific view key and they can eliminate 255 out of 256 potential outputs. So they massively reduce the amount of work you need to do, but they can't specifically find exactly which outputs are yours, and you still maintain a large degree of privacy.

The downside to this trusted third party, who is able to reduce how many outputs you have to scan, is they do actually have to for each key they're scanning for, go through every output. We're still discussing this linear amount of work.

So, if we introduce fully homomorphic encryption, because why not throw more numbers at it, with the same theoretical complexity, they could actually do the full scan process without any loss of privacy. Instead of saying: oh yeah, I did 250 fifths of 256 of the work, you do the rest. They can say: no, I scanned the entire blockchain for you, here's the exact outputs that are yours, without being able to tell which outputs those are. So it would be this really powerful primitive, and it would not increase the theoretical complexity. For each key they're scanning for, they still have to apply the algorithm to each of the outputs.

Oh, as one other note, it can also be done entirely outside of the Monero protocol. Someone today can build a wallet that does this and just go forward with it.

Bad news, fully homomorphic encryption very slow. Very slow. In 2007, first game posited, 30 seconds of binary operation. We do a lot of binary operations in the scanning process. Computers do a lot of those by default. And in order for this to really be viable, it has to be faster to scan an output that you can feasibly create an output on the Monero blockchain, else you form an infinitely growing backlog.

There are more efficient protocols now, and I'm not sure if we're at that break-even point or within a couple orders of magnitude of it. If we are within a couple orders of magnitude, like this actually could be something we could start discussing, especially with USB accelerations, such as FPGAs that you communicate with over USB. But that still doesn't change. It would be a lot of work to develop and implement. And then this third party could censor transactions, unless they also do a zero-knowledge proof, which would also be of significant computational complexity.

And the biggest issue, in my opinion, is that as we go for these more efficient protocols, they unfortunately would be broken in the future. And if they're broken in the future and revealed to not be private, anyone who relied on them now will lose their privacy. So we have to be very careful that we're not just going for the latest and greatest protocol, but we're still discussing something secure.

For my kind of final idea of this is a problem within Monero, we can shove numbers at it and fix it, but we can't actually, I actually wanted to discuss sending when you're using an untrusted RPC. This means you're connecting to a remote blockchain node, and you don't trust it. It literally could be the NSA trying to learn more information about you, but you're still sending your transactions through them and expecting privacy.

So right now, from Monero, we use ring signatures, and in order to successfully create a ring signature, you need your output and 15 decoy outputs, and you need the information on every single output. So right now, your Monero wallet will request information on all 16 outputs. It only needs the information for 15 of those, because one of those is its output, and it already has that information. But if you only requested information on 15 outputs, and then one minute later, a transaction appears on chain with those 15 outputs and one other, that server immediately can see: huh, that's an odd output out. Someone requested all information on every single other output, but not that one. Why is that one special? And conclude it's the output actually being spent. And full chain membership proofs do replace rings, but this concept kinda is still around.

So we actually can fix this, and this one actually does end up pretty viable. We could use private information retrieval, also known as PIR, in order to request output information or under full chain membership proofs, membership paths, without the server learning what info was requested. It's basically server has a bunch of information, you can request some of it, server cannot tell which information it just responded with. Despite the server being the one to tell you the information, it has no idea what information it just gave you, nor what you requested.

And this actually does achieve logarithmic complexity, which is what we would need. We cannot have an algorithm that, for 100 million outputs has to iterate over every single one of our 100 million outputs in order to return a response. With the logarithmic complexity, it would iterate over a few thousand, I believe. There's been a variety of these schemes discussed over the years, so this actually is a large-standing field of theory. And there's even this very great demonstration from several years ago, and because it's now probably from four or five years ago, it just goes to show further improvements on theory slowly get better. But Spiral Wiki, you can browse English Wikipedia without the server knowing which page you're on. No matter what page you request, it can't tell which page you're on, and it's a really great demonstration.

So kind of like all of the fully homomorphic encryption drawbacks, it's slow, and it would notably increase the CPU requirements, because now instead of just saying: hey, I want information on this output. Oh, yeah, let me get it from the database and respond. It's, hey, I want information on some output. Here's an encrypted blob of it. And the server goes: ok, let me run all of this cryptography to figure out the answer and then tell you it while it's still encrypted. So it is slow, but if you go on Spiral Wiki, it truly is a great demo, their server, which is just for demonstration purposes, it's not at all at an industrial capacity, can actually respond with full Wikipedia pages, text only, within just a couple of seconds. And I truly think it is a demonstration of the potential of this technology, especially as it's a rapidly evolving field for its widespread applicability to a variety of hard problems. It has the same privacy risks. If we rely on PIR, and then that PIR scheme is proved to not actually be private information retrieval. We're kind of screwed over there out of privacy, so we do have to be careful. But I actually do think it is something that we should start considering and working towards as a long-term goal because it likely would decrease bandwidth usage and offer stronger privacy in the context of full-chain membership proofs.

I'm not sure if we have time for questions, but that was kind of my smorgasbord of interesting issues and weird cryptography we can do to work around it. If we do have time, I'm open to questions. If we don't, I will have to recall it.
